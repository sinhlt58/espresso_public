
config:
  # ES indexer bolt
  # adresses can be specified as a full URL
  # if not we assume that the protocol is http and the port 9200
  es.indexer.addresses: "localhost"
  es.indexer.index.name: "fb_index"
  # es.indexer.pipeline: "_PIPELINE_"
  es.indexer.doc.type: "_doc"
  es.indexer.create: false
  es.indexer.settings:
    cluster.name: "elasticsearch"

  # ES spout and persistence bolt
  es.status.addresses: "http://localhost:9200"
  es.status.index.name: "fb_status"
  es.status.doc.type: "_doc"

  # ES spout get brand and index to Status
  es.brand.addresses: "http://103.220.68.79:9200"
  es.brand.index.name: "v2_analysis"
  es.brand.doc.type: "_doc"

  #es.status.user: "USERNAME"
  #es.status.password: "PASSWORD"
  # the routing is done on the value of 'partition.node.mode'
  es.status.routing: true
  # stores the value used for the routing as a separate field
  # needed by the spout implementations
  es.status.routing.fieldname: "metadata.type"
  es.status.bulkActions: 500
  es.status.flushInterval: "2s"
  es.status.concurrentRequests: 3
  es.status.settings:
    cluster.name: "elasticsearch"

  ################
  # spout config #
  ################
  
  # positive or negative filter parsable by the Lucene Query Parser
  # es.status.filterQuery: "-(metadata.hostname:stormcrawler.net)"

  # time in secs for which the Nodes will be considered for fetching after a ack of fail
  spout.ttl.purgatory: 30
  
  # Min time (in msecs) to allow between 2 successive queries to ES
  spout.min.delay.queries: 3000

  # Delay since previous query date (in secs) after which the nextFetchDate value will be reset to the current time
  # Setting this to -1 or a large value means that the ES will cache the results but also that less and less results
  # might be returned.
  spout.reset.fetchdate.after: 120

  es.status.max.buckets: 3
  es.status.max.nodes.per.bucket: 3
  # field to group the Nodes into buckets
  es.status.bucket.field: "metadata.type"
  # field to sort the Nodes within a bucket
  es.status.bucket.sort.field: "nextFetchDate"
  # field to sort the buckets
  es.status.global.sort.field: "nextFetchDate"

  # CollapsingSpout : limits the deep paging by resetting the start offset for the ES query 
  # es.status.max.start.offset: 500
  
  # AggregationSpout : sampling improves the performance on large crawls
  es.status.sample: false

  # AggregationSpout (expert): adds this value in mins to the latest date returned in the results and
  # use it as nextFetchDate
  es.status.recentDate.increase: -1
  es.status.recentDate.min.gap: -1


  # Config for spout brand
  # time in secs for which the Nodes will be considered for fetching after a ack of fail
  spout.brand.ttl.purgatory: 30
  
  # Min time (in msecs) to allow between 2 successive queries to ES
  spout.brand.min.delay.queries: 18000000

  spout.brand.DBMaker.file.name: "/espresso_data/timeLastGetBrand.db"

  spout.brand.time.last.get.default: 1546406121802
  
  spout.brand.next.time: 18000000

  spout.brand.field.time: "time"

  spout.brand.field.range: "createdTime"

  spout.brand.field.term: "itemType"
  
  spout.brand.value.file.term: "product"

  # ignore brand name (include null and length of brand name greater 1)
  spout.brand.value.ignore: "no brand, 0"

  es.brand.max.buckets: 10000
  # field to group the Nodes into buckets
  es.brand.bucket.fields: "brand.keyword,author.keyword"
  
  # AggregationBrandSpout : sampling improves the performance on large crawls
  es.brand.sample: false


